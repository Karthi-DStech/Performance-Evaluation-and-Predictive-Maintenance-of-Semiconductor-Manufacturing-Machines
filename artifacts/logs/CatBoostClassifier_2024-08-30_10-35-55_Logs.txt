==========================================================================================
          Semiconductor Manufacturing Unit (SECOM) Failure Prediction LOG REPORT          
==========================================================================================

Log created at: 2024-08-30 10:30:22

Model Utilised for the Experiment: CatBoostClassifier

==========================================================================================
DATA LOADING:


  Data loaded: True


  Total samples: 1567


  Total features: 592


  Missing values: Yes


  Dataframe info: <class 'pandas.core.frame.DataFrame'>
RangeIndex: 1567 entries, 0 to 1566
Columns: 592 entries, Time to Pass/Fail
dtypes: float64(590), int64(1), object(1)
memory usage: 7.1+ MB


--------------------------------------------------------------------------------
DATA PROCESSING:


  Missing values:

    Time: 0
    0: 6
    1: 7
    2: 14
    3: 14
    4: 14
    5: 14
    6: 14
    7: 9
    8: 2
    9: 2
    10: 2
    11: 2
    12: 2
    13: 3
    14: 3
    15: 3
    16: 3
    17: 3
    18: 3
    19: 10
    20: 0
    21: 2
    22: 2
    23: 2
    24: 2
    25: 2
    26: 2
    27: 2
    28: 2
    29: 2
    30: 2
    31: 2
    32: 1
    33: 1
    34: 1
    35: 1
    36: 1
    37: 1
    38: 1
    39: 1
    40: 24
    41: 24
    42: 1
    43: 1
    44: 1
    45: 1
    46: 1
    47: 1
    48: 1
    49: 1
    50: 1
    51: 1
    52: 1
    53: 4
    54: 4
    55: 4
    56: 4
    57: 4
    58: 4
    59: 7
    60: 6
    61: 6
    62: 6
    63: 7
    64: 7
    65: 7
    66: 6
    67: 6
    68: 6
    69: 6
    70: 6
    71: 6
    72: 794
    73: 794
    74: 6
    75: 24
    76: 24
    77: 24
    78: 24
    79: 24
    80: 24
    81: 24
    82: 24
    83: 1
    84: 12
    85: 1341
    86: 0
    87: 0
    88: 0
    89: 51
    90: 51
    91: 6
    92: 2
    93: 2
    94: 6
    95: 6
    96: 6
    97: 6
    98: 6
    99: 6
    100: 6
    101: 6
    102: 6
    103: 2
    104: 2
    105: 6
    106: 6
    107: 6
    108: 6
    109: 1018
    110: 1018
    111: 1018
    112: 715
    113: 0
    114: 0
    115: 0
    116: 0
    117: 0
    118: 24
    119: 0
    120: 0
    121: 9
    122: 9
    123: 9
    124: 9
    125: 9
    126: 9
    127: 9
    128: 9
    129: 9
    130: 9
    131: 9
    132: 8
    133: 8
    134: 8
    135: 5
    136: 6
    137: 7
    138: 14
    139: 14
    140: 14
    141: 14
    142: 14
    143: 9
    144: 2
    145: 2
    146: 2
    147: 2
    148: 2
    149: 3
    150: 3
    151: 3
    152: 3
    153: 3
    154: 3
    155: 10
    156: 0
    157: 1429
    158: 1429
    159: 2
    160: 2
    161: 2
    162: 2
    163: 2
    164: 2
    165: 2
    166: 2
    167: 2
    168: 2
    169: 2
    170: 1
    171: 1
    172: 1
    173: 1
    174: 1
    175: 1
    176: 1
    177: 1
    178: 24
    179: 1
    180: 1
    181: 1
    182: 1
    183: 1
    184: 1
    185: 1
    186: 1
    187: 1
    188: 1
    189: 1
    190: 4
    191: 4
    192: 4
    193: 4
    194: 4
    195: 4
    196: 7
    197: 6
    198: 6
    199: 6
    200: 7
    201: 7
    202: 7
    203: 6
    204: 6
    205: 6
    206: 6
    207: 6
    208: 6
    209: 6
    210: 24
    211: 24
    212: 24
    213: 24
    214: 24
    215: 24
    216: 24
    217: 24
    218: 1
    219: 12
    220: 1341
    221: 0
    222: 0
    223: 0
    224: 51
    225: 51
    226: 6
    227: 2
    228: 2
    229: 6
    230: 6
    231: 6
    232: 6
    233: 6
    234: 6
    235: 6
    236: 6
    237: 6
    238: 2
    239: 2
    240: 6
    241: 6
    242: 6
    243: 6
    244: 1018
    245: 1018
    246: 1018
    247: 715
    248: 0
    249: 0
    250: 0
    251: 0
    252: 0
    253: 24
    254: 0
    255: 0
    256: 9
    257: 9
    258: 9
    259: 9
    260: 9
    261: 9
    262: 9
    263: 9
    264: 9
    265: 9
    266: 9
    267: 8
    268: 8
    269: 8
    270: 5
    271: 6
    272: 7
    273: 14
    274: 14
    275: 14
    276: 14
    277: 14
    278: 9
    279: 2
    280: 2
    281: 2
    282: 2
    283: 2
    284: 3
    285: 3
    286: 3
    287: 3
    288: 3
    289: 3
    290: 10
    291: 0
    292: 1429
    293: 1429
    294: 2
    295: 2
    296: 2
    297: 2
    298: 2
    299: 2
    300: 2
    301: 2
    302: 2
    303: 2
    304: 2
    305: 1
    306: 1
    307: 1
    308: 1
    309: 1
    310: 1
    311: 1
    312: 1
    313: 24
    314: 24
    315: 1
    316: 1
    317: 1
    318: 1
    319: 1
    320: 1
    321: 1
    322: 1
    323: 1
    324: 1
    325: 1
    326: 4
    327: 4
    328: 4
    329: 4
    330: 4
    331: 4
    332: 7
    333: 6
    334: 6
    335: 6
    336: 7
    337: 7
    338: 7
    339: 6
    340: 6
    341: 6
    342: 6
    343: 6
    344: 6
    345: 794
    346: 794
    347: 6
    348: 24
    349: 24
    350: 24
    351: 24
    352: 24
    353: 24
    354: 24
    355: 24
    356: 1
    357: 12
    358: 1341
    359: 0
    360: 0
    361: 0
    362: 51
    363: 51
    364: 6
    365: 2
    366: 2
    367: 6
    368: 6
    369: 6
    370: 6
    371: 6
    372: 6
    373: 6
    374: 6
    375: 6
    376: 2
    377: 2
    378: 6
    379: 6
    380: 6
    381: 6
    382: 1018
    383: 1018
    384: 1018
    385: 715
    386: 0
    387: 0
    388: 0
    389: 0
    390: 0
    391: 24
    392: 0
    393: 0
    394: 9
    395: 9
    396: 9
    397: 9
    398: 9
    399: 9
    400: 9
    401: 9
    402: 9
    403: 9
    404: 9
    405: 8
    406: 8
    407: 8
    408: 5
    409: 6
    410: 7
    411: 14
    412: 14
    413: 14
    414: 14
    415: 14
    416: 9
    417: 2
    418: 2
    419: 2
    420: 2
    421: 2
    422: 3
    423: 3
    424: 3
    425: 3
    426: 3
    427: 3
    428: 10
    429: 0
    430: 2
    431: 2
    432: 2
    433: 2
    434: 2
    435: 2
    436: 2
    437: 2
    438: 2
    439: 2
    440: 2
    441: 1
    442: 1
    443: 1
    444: 1
    445: 1
    446: 1
    447: 1
    448: 1
    449: 24
    450: 24
    451: 1
    452: 1
    453: 1
    454: 1
    455: 1
    456: 1
    457: 1
    458: 1
    459: 1
    460: 1
    461: 1
    462: 4
    463: 4
    464: 4
    465: 4
    466: 4
    467: 4
    468: 7
    469: 6
    470: 6
    471: 6
    472: 7
    473: 7
    474: 7
    475: 6
    476: 6
    477: 6
    478: 6
    479: 6
    480: 6
    481: 6
    482: 24
    483: 24
    484: 24
    485: 24
    486: 24
    487: 24
    488: 24
    489: 24
    490: 1
    491: 12
    492: 1341
    493: 0
    494: 0
    495: 0
    496: 51
    497: 51
    498: 6
    499: 2
    500: 2
    501: 6
    502: 6
    503: 6
    504: 6
    505: 6
    506: 6
    507: 6
    508: 6
    509: 6
    510: 2
    511: 2
    512: 6
    513: 6
    514: 6
    515: 6
    516: 1018
    517: 1018
    518: 1018
    519: 715
    520: 0
    521: 0
    522: 0
    523: 0
    524: 0
    525: 24
    526: 0
    527: 0
    528: 9
    529: 9
    530: 9
    531: 9
    532: 9
    533: 9
    534: 9
    535: 9
    536: 9
    537: 9
    538: 9
    539: 8
    540: 8
    541: 8
    542: 2
    543: 2
    544: 2
    545: 2
    546: 260
    547: 260
    548: 260
    549: 260
    550: 260
    551: 260
    552: 260
    553: 260
    554: 260
    555: 260
    556: 260
    557: 260
    558: 1
    559: 1
    560: 1
    561: 1
    562: 273
    563: 273
    564: 273
    565: 273
    566: 273
    567: 273
    568: 273
    569: 273
    570: 0
    571: 0
    572: 0
    573: 0
    574: 0
    575: 0
    576: 0
    577: 0
    578: 949
    579: 949
    580: 949
    581: 949
    582: 1
    583: 1
    584: 1
    585: 1
    586: 1
    587: 1
    588: 1
    589: 1
    Pass/Fail: 0


  Categorical columns: ['Time']


  Numeric columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', 'Pass/Fail']


  Other columns: None


  Imputation method type: global_imputation


  Imputation methods: fillna


  Feature engineering: Date columns separated


  Dropped columns: ['Time']


  Final number of columns: 597


  Final columns: Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
       ...
       '587', '588', '589', 'Pass/Fail', 'Year', 'Month', 'Day', 'Hour',
       'Day of Week', 'Minute'],
      dtype='object', length=597)

--------------------------------------------------------------------------------
TRAIN TEST SPLIT:


  Target column: Pass/Fail


  Test size: 0.3


  Train size: 1096


  Test size count: 471


  Scaling: StandardScaler


  Checks: All checks passed successfully

--------------------------------------------------------------------------------
MODEL TRAINING:


  Model name: CatBoostClassifier


  Training started: 2024-08-30 10:30:23


  Training completed: 2024-08-30 10:30:37


  Training duration: 13.72 seconds


  Status: completed


  Feature importance calculation status: started


  Feature importance technique: Plot Tree Importance


  Feature importance calculation: completed


  Number of features selected: 35


  Feature importance: 
+-----+---------+---------------------+
|     | Feature |     Importance      |
+-----+---------+---------------------+
| 59  |   59    | 3.9912265977903227  |
| 64  |   64    | 1.1430728906195213  |
| 247 |   247   | 1.1367907776354478  |
| 500 |   500   |  1.055645726518531  |
| 541 |   541   | 0.9286524337628448  |
| 519 |   519   | 0.8825284981227952  |
| 129 |   129   | 0.8266093604095207  |
| 153 |   153   | 0.7664979637440184  |
| 16  |   16    | 0.7372958723765726  |
| 348 |   348   | 0.7233653861597764  |
| 28  |   28    | 0.5879104074289192  |
| 333 |   333   | 0.5671310850305613  |
| 65  |   65    | 0.5455516377466897  |
| 477 |   477   | 0.5409983497908827  |
| 33  |   33    | 0.5345236057048814  |
| 587 |   587   | 0.5275264681060485  |
| 146 |   146   |  0.502255971986684  |
| 25  |   25    | 0.4939137437812106  |
| 411 |   411   | 0.4925008177159222  |
| 117 |   117   | 0.4903667267332303  |
| 227 |   227   | 0.4624666061586549  |
| 40  |   40    | 0.45933904913405604 |
| 21  |   21    | 0.4570163589643703  |
| 223 |   223   | 0.45164291451735333 |
| 73  |   73    | 0.43977051183090643 |
| 102 |   102   | 0.43119421493301724 |
| 346 |   346   | 0.4274866747247031  |
| 575 |   575   | 0.42533622674044846 |
| 361 |   361   | 0.4172006967822691  |
| 210 |   210   | 0.41312899920656837 |
| 582 |   582   | 0.4103889193316854  |
| 312 |   312   | 0.4091290704979578  |
| 304 |   304   | 0.40745796431400805 |
| 26  |   26    | 0.40668639199932133 |
| 122 |   122   | 0.4052104787844994  |
+-----+---------+---------------------+


--------------------------------------------------------------------------------
MODEL EVALUATION:


  Accuracy: 0.9278131634819533


  Classification report:               precision    recall  f1-score   support

          -1       0.93      1.00      0.96       437
           1       0.00      0.00      0.00        34

    accuracy                           0.93       471
   macro avg       0.46      0.50      0.48       471
weighted avg       0.86      0.93      0.89       471


--------------------------------------------------------------------------------
MODEL TUNING:


  Tuning started: 2024-08-30 10:30:37


  Method: Optuna for hyperparameter tuning


  N trials: 100


  Best hyperparameters:

    iterations: 66
    learning_rate: 0.33141105262570647
    depth: 6
    l2_leaf_reg: 0.5086651897086711
    border_count: 182
    bagging_temperature: 0.7627538214745395
    random_strength: 0.25390704326162267


  Tuning completed: 2024-08-30 10:35:55


  Tuning duration: 317.98 seconds


  Status: completed


  Feature importance calculation status: started


  Feature importance technique: Plot Tree Importance


  Feature importance calculation: completed


  Number of features selected: 35


  Feature importance: 
+-----+---------+--------------------+
|     | Feature |     Importance     |
+-----+---------+--------------------+
| 117 |   117   | 4.055107295897892  |
| 287 |   287   | 3.770065843854989  |
| 154 |   154   | 3.7075449129661378 |
| 59  |   59    | 3.4443924944636923 |
| 582 |   582   | 3.1401773437211262 |
| 111 |   111   | 2.987693952994863  |
| 64  |   64    | 2.5274690315252744 |
| 65  |   65    | 2.186698814419027  |
| 500 |   500   | 1.8960991492800356 |
| 304 |   304   | 1.7760689233526445 |
| 119 |   119   | 1.6539675835872807 |
| 21  |   21    | 1.6218384225869273 |
| 132 |   132   | 1.6218369070583483 |
| 55  |   55    | 1.4467738862900614 |
| 489 |   489   | 1.3684654494504884 |
|  6  |    6    | 1.321769920436124  |
| 341 |   341   | 1.295251961933629  |
| 70  |   70    | 1.2810005899438177 |
| 348 |   348   | 1.2761085349399635 |
| 426 |   426   | 1.2411643513912438 |
| 44  |   44    | 1.2295854311077665 |
| 195 |   195   |  1.22229644745042  |
| 495 |   495   | 1.175667241703828  |
| 103 |   103   | 1.141726856730883  |
| 153 |   153   | 1.108104086628875  |
| 519 |   519   | 1.0558506201637923 |
| 488 |   488   | 1.0483692434484306 |
| 366 |   366   | 0.9740356992919587 |
| 416 |   416   | 0.9364225726428608 |
| 28  |   28    | 0.9174816279896469 |
| 77  |   77    | 0.9173783535872226 |
| 282 |   282   | 0.8775359007740382 |
| 420 |   420   | 0.8733002890773656 |
| 78  |   78    | 0.8561597879791276 |
| 595 | Minute  | 0.8363935621489557 |
+-----+---------+--------------------+


--------------------------------------------------------------------------------
MODEL TUNING EVALUATION:


  Accuracy: 0.9320594479830149


  Classification report:               precision    recall  f1-score   support

          -1       0.93      1.00      0.96       437
           1       1.00      0.06      0.11        34

    accuracy                           0.93       471
   macro avg       0.97      0.53      0.54       471
weighted avg       0.94      0.93      0.90       471


--------------------------------------------------------------------------------
MODEL SAVING:


  Model path: /Users/Barathi/SECOM-Dissertation-CODE/artifacts/models/CatBoostClassifier_2024-08-30_10-35-55_model.pkl

--------------------------------------------------------------------------------
SFS:


  Status: Skipped

--------------------------------------------------------------------------------
================================================================================