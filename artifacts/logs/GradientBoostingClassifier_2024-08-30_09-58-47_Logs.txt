==========================================================================================
          Semiconductor Manufacturing Unit (SECOM) Failure Prediction LOG REPORT          
==========================================================================================

Log created at: 2024-08-30 09:26:22

Model Utilised for the Experiment: GradientBoostingClassifier

==========================================================================================
DATA LOADING:


  Data loaded: True


  Total samples: 1567


  Total features: 592


  Missing values: Yes


  Dataframe info: <class 'pandas.core.frame.DataFrame'>
RangeIndex: 1567 entries, 0 to 1566
Columns: 592 entries, Time to Pass/Fail
dtypes: float64(590), int64(1), object(1)
memory usage: 7.1+ MB


--------------------------------------------------------------------------------
DATA PROCESSING:


  Missing values:

    Time: 0
    0: 6
    1: 7
    2: 14
    3: 14
    4: 14
    5: 14
    6: 14
    7: 9
    8: 2
    9: 2
    10: 2
    11: 2
    12: 2
    13: 3
    14: 3
    15: 3
    16: 3
    17: 3
    18: 3
    19: 10
    20: 0
    21: 2
    22: 2
    23: 2
    24: 2
    25: 2
    26: 2
    27: 2
    28: 2
    29: 2
    30: 2
    31: 2
    32: 1
    33: 1
    34: 1
    35: 1
    36: 1
    37: 1
    38: 1
    39: 1
    40: 24
    41: 24
    42: 1
    43: 1
    44: 1
    45: 1
    46: 1
    47: 1
    48: 1
    49: 1
    50: 1
    51: 1
    52: 1
    53: 4
    54: 4
    55: 4
    56: 4
    57: 4
    58: 4
    59: 7
    60: 6
    61: 6
    62: 6
    63: 7
    64: 7
    65: 7
    66: 6
    67: 6
    68: 6
    69: 6
    70: 6
    71: 6
    72: 794
    73: 794
    74: 6
    75: 24
    76: 24
    77: 24
    78: 24
    79: 24
    80: 24
    81: 24
    82: 24
    83: 1
    84: 12
    85: 1341
    86: 0
    87: 0
    88: 0
    89: 51
    90: 51
    91: 6
    92: 2
    93: 2
    94: 6
    95: 6
    96: 6
    97: 6
    98: 6
    99: 6
    100: 6
    101: 6
    102: 6
    103: 2
    104: 2
    105: 6
    106: 6
    107: 6
    108: 6
    109: 1018
    110: 1018
    111: 1018
    112: 715
    113: 0
    114: 0
    115: 0
    116: 0
    117: 0
    118: 24
    119: 0
    120: 0
    121: 9
    122: 9
    123: 9
    124: 9
    125: 9
    126: 9
    127: 9
    128: 9
    129: 9
    130: 9
    131: 9
    132: 8
    133: 8
    134: 8
    135: 5
    136: 6
    137: 7
    138: 14
    139: 14
    140: 14
    141: 14
    142: 14
    143: 9
    144: 2
    145: 2
    146: 2
    147: 2
    148: 2
    149: 3
    150: 3
    151: 3
    152: 3
    153: 3
    154: 3
    155: 10
    156: 0
    157: 1429
    158: 1429
    159: 2
    160: 2
    161: 2
    162: 2
    163: 2
    164: 2
    165: 2
    166: 2
    167: 2
    168: 2
    169: 2
    170: 1
    171: 1
    172: 1
    173: 1
    174: 1
    175: 1
    176: 1
    177: 1
    178: 24
    179: 1
    180: 1
    181: 1
    182: 1
    183: 1
    184: 1
    185: 1
    186: 1
    187: 1
    188: 1
    189: 1
    190: 4
    191: 4
    192: 4
    193: 4
    194: 4
    195: 4
    196: 7
    197: 6
    198: 6
    199: 6
    200: 7
    201: 7
    202: 7
    203: 6
    204: 6
    205: 6
    206: 6
    207: 6
    208: 6
    209: 6
    210: 24
    211: 24
    212: 24
    213: 24
    214: 24
    215: 24
    216: 24
    217: 24
    218: 1
    219: 12
    220: 1341
    221: 0
    222: 0
    223: 0
    224: 51
    225: 51
    226: 6
    227: 2
    228: 2
    229: 6
    230: 6
    231: 6
    232: 6
    233: 6
    234: 6
    235: 6
    236: 6
    237: 6
    238: 2
    239: 2
    240: 6
    241: 6
    242: 6
    243: 6
    244: 1018
    245: 1018
    246: 1018
    247: 715
    248: 0
    249: 0
    250: 0
    251: 0
    252: 0
    253: 24
    254: 0
    255: 0
    256: 9
    257: 9
    258: 9
    259: 9
    260: 9
    261: 9
    262: 9
    263: 9
    264: 9
    265: 9
    266: 9
    267: 8
    268: 8
    269: 8
    270: 5
    271: 6
    272: 7
    273: 14
    274: 14
    275: 14
    276: 14
    277: 14
    278: 9
    279: 2
    280: 2
    281: 2
    282: 2
    283: 2
    284: 3
    285: 3
    286: 3
    287: 3
    288: 3
    289: 3
    290: 10
    291: 0
    292: 1429
    293: 1429
    294: 2
    295: 2
    296: 2
    297: 2
    298: 2
    299: 2
    300: 2
    301: 2
    302: 2
    303: 2
    304: 2
    305: 1
    306: 1
    307: 1
    308: 1
    309: 1
    310: 1
    311: 1
    312: 1
    313: 24
    314: 24
    315: 1
    316: 1
    317: 1
    318: 1
    319: 1
    320: 1
    321: 1
    322: 1
    323: 1
    324: 1
    325: 1
    326: 4
    327: 4
    328: 4
    329: 4
    330: 4
    331: 4
    332: 7
    333: 6
    334: 6
    335: 6
    336: 7
    337: 7
    338: 7
    339: 6
    340: 6
    341: 6
    342: 6
    343: 6
    344: 6
    345: 794
    346: 794
    347: 6
    348: 24
    349: 24
    350: 24
    351: 24
    352: 24
    353: 24
    354: 24
    355: 24
    356: 1
    357: 12
    358: 1341
    359: 0
    360: 0
    361: 0
    362: 51
    363: 51
    364: 6
    365: 2
    366: 2
    367: 6
    368: 6
    369: 6
    370: 6
    371: 6
    372: 6
    373: 6
    374: 6
    375: 6
    376: 2
    377: 2
    378: 6
    379: 6
    380: 6
    381: 6
    382: 1018
    383: 1018
    384: 1018
    385: 715
    386: 0
    387: 0
    388: 0
    389: 0
    390: 0
    391: 24
    392: 0
    393: 0
    394: 9
    395: 9
    396: 9
    397: 9
    398: 9
    399: 9
    400: 9
    401: 9
    402: 9
    403: 9
    404: 9
    405: 8
    406: 8
    407: 8
    408: 5
    409: 6
    410: 7
    411: 14
    412: 14
    413: 14
    414: 14
    415: 14
    416: 9
    417: 2
    418: 2
    419: 2
    420: 2
    421: 2
    422: 3
    423: 3
    424: 3
    425: 3
    426: 3
    427: 3
    428: 10
    429: 0
    430: 2
    431: 2
    432: 2
    433: 2
    434: 2
    435: 2
    436: 2
    437: 2
    438: 2
    439: 2
    440: 2
    441: 1
    442: 1
    443: 1
    444: 1
    445: 1
    446: 1
    447: 1
    448: 1
    449: 24
    450: 24
    451: 1
    452: 1
    453: 1
    454: 1
    455: 1
    456: 1
    457: 1
    458: 1
    459: 1
    460: 1
    461: 1
    462: 4
    463: 4
    464: 4
    465: 4
    466: 4
    467: 4
    468: 7
    469: 6
    470: 6
    471: 6
    472: 7
    473: 7
    474: 7
    475: 6
    476: 6
    477: 6
    478: 6
    479: 6
    480: 6
    481: 6
    482: 24
    483: 24
    484: 24
    485: 24
    486: 24
    487: 24
    488: 24
    489: 24
    490: 1
    491: 12
    492: 1341
    493: 0
    494: 0
    495: 0
    496: 51
    497: 51
    498: 6
    499: 2
    500: 2
    501: 6
    502: 6
    503: 6
    504: 6
    505: 6
    506: 6
    507: 6
    508: 6
    509: 6
    510: 2
    511: 2
    512: 6
    513: 6
    514: 6
    515: 6
    516: 1018
    517: 1018
    518: 1018
    519: 715
    520: 0
    521: 0
    522: 0
    523: 0
    524: 0
    525: 24
    526: 0
    527: 0
    528: 9
    529: 9
    530: 9
    531: 9
    532: 9
    533: 9
    534: 9
    535: 9
    536: 9
    537: 9
    538: 9
    539: 8
    540: 8
    541: 8
    542: 2
    543: 2
    544: 2
    545: 2
    546: 260
    547: 260
    548: 260
    549: 260
    550: 260
    551: 260
    552: 260
    553: 260
    554: 260
    555: 260
    556: 260
    557: 260
    558: 1
    559: 1
    560: 1
    561: 1
    562: 273
    563: 273
    564: 273
    565: 273
    566: 273
    567: 273
    568: 273
    569: 273
    570: 0
    571: 0
    572: 0
    573: 0
    574: 0
    575: 0
    576: 0
    577: 0
    578: 949
    579: 949
    580: 949
    581: 949
    582: 1
    583: 1
    584: 1
    585: 1
    586: 1
    587: 1
    588: 1
    589: 1
    Pass/Fail: 0


  Categorical columns: ['Time']


  Numeric columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', 'Pass/Fail']


  Other columns: None


  Imputation method type: global_imputation


  Imputation methods: fillna


  Feature engineering: Date columns separated


  Dropped columns: ['Time']


  Final number of columns: 597


  Final columns: Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
       ...
       '587', '588', '589', 'Pass/Fail', 'Year', 'Month', 'Day', 'Hour',
       'Day of Week', 'Minute'],
      dtype='object', length=597)

--------------------------------------------------------------------------------
TRAIN TEST SPLIT:


  Target column: Pass/Fail


  Test size: 0.3


  Train size: 1096


  Test size count: 471


  Scaling: StandardScaler


  Checks: All checks passed successfully

--------------------------------------------------------------------------------
MODEL TRAINING:


  Model name: GradientBoostingClassifier


  Training started: 2024-08-30 09:26:22


  Training completed: 2024-08-30 09:26:30


  Training duration: 7.90 seconds


  Status: completed


  Feature importance calculation status: started


  Feature importance technique: Plot Tree Importance


  Feature importance calculation: completed


  Number of features selected: 35


  Feature importance: 
+-----+---------+----------------------+
|     | Feature |      Importance      |
+-----+---------+----------------------+
| 64  |   64    | 0.04970818124660647  |
| 59  |   59    | 0.04442048462722256  |
| 99  |   99    | 0.02476512572163363  |
| 441 |   441   | 0.02410444939043987  |
| 269 |   269   | 0.02295535040513189  |
| 28  |   28    | 0.02111688184086664  |
| 40  |   40    | 0.019978435162889937 |
| 500 |   500   | 0.019942242152134037 |
| 541 |   541   | 0.019284056679920808 |
| 25  |   25    | 0.018823991225943325 |
| 333 |   333   | 0.01832521786722238  |
| 295 |   295   | 0.01729304645777897  |
| 289 |   289   | 0.016995445866851224 |
| 18  |   18    | 0.015382475485724703 |
| 299 |   299   | 0.015137511531420272 |
| 77  |   77    | 0.014469572575733513 |
| 132 |   132   | 0.01427582282205005  |
| 78  |   78    | 0.014168203725634473 |
| 585 |   585   | 0.013583129864637562 |
| 117 |   117   | 0.013570830696983975 |
| 365 |   365   | 0.012722635824212554 |
| 51  |   51    | 0.012640083861391304 |
| 16  |   16    | 0.01237935601448289  |
| 114 |   114   | 0.012206835845813566 |
| 103 |   103   | 0.011633788161317144 |
|  4  |    4    | 0.010931597691665683 |
| 406 |   406   | 0.010931262632946685 |
| 168 |   168   | 0.010511885448912675 |
|  7  |    7    | 0.010388645499236427 |
| 582 |   582   | 0.010213178720533175 |
| 301 |   301   | 0.01016796539438872  |
| 359 |   359   | 0.009342375097995915 |
| 523 |   523   | 0.009151411648715286 |
| 519 |   519   | 0.008950097089682151 |
| 38  |   38    | 0.008925211851050012 |
+-----+---------+----------------------+


--------------------------------------------------------------------------------
MODEL EVALUATION:


  Accuracy: 0.9235668789808917


  Classification report:               precision    recall  f1-score   support

          -1       0.93      0.99      0.96       437
           1       0.25      0.03      0.05        34

    accuracy                           0.92       471
   macro avg       0.59      0.51      0.51       471
weighted avg       0.88      0.92      0.89       471


--------------------------------------------------------------------------------
MODEL TUNING:


  Tuning started: 2024-08-30 09:26:30


  Method: Optuna for hyperparameter tuning


  N trials: 100


  Best hyperparameters:

    n_estimators: 184
    learning_rate: 0.08664009981444973
    max_depth: 8
    min_samples_split: 7
    min_samples_leaf: 4
    max_features: None
    subsample: 0.9448934534080029


  Tuning completed: 2024-08-30 09:58:47


  Tuning duration: 1937.60 seconds


  Status: completed


  Feature importance calculation status: started


  Feature importance technique: Plot Tree Importance


  Feature importance calculation: completed


  Number of features selected: 35


  Feature importance: 
+-----+---------+-----------------------+
|     | Feature |      Importance       |
+-----+---------+-----------------------+
| 59  |   59    |  0.03899666123494352  |
| 64  |   64    |  0.02732500676149823  |
| 304 |   304   | 0.017897053067574616  |
| 333 |   333   | 0.017846744948142104  |
| 558 |   558   | 0.016867999174541282  |
| 132 |   132   | 0.016100770619156247  |
| 25  |   25    | 0.015907707961154788  |
| 356 |   356   | 0.015346195457653021  |
| 55  |   55    |  0.01291908633900784  |
| 541 |   541   | 0.012680975410581167  |
| 348 |   348   |  0.01253326313316587  |
| 331 |   331   | 0.012145887000264895  |
| 386 |   386   | 0.011911607310459801  |
| 216 |   216   | 0.011871967462698486  |
| 500 |   500   | 0.011821725415477456  |
| 499 |   499   |  0.01141550495839423  |
| 61  |   61    | 0.010672225857255848  |
| 63  |   63    | 0.010349506398904673  |
| 100 |   100   | 0.010169151491082813  |
| 16  |   16    | 0.009715258830219913  |
| 78  |   78    | 0.009329442567334567  |
| 436 |   436   | 0.009323165515283206  |
| 303 |   303   | 0.009126302859511356  |
| 99  |   99    | 0.009083664973444445  |
| 487 |   487   | 0.008952372830569173  |
| 159 |   159   | 0.008526759491864815  |
| 460 |   460   | 0.008269768756276656  |
| 54  |   54    | 0.008189645432461378  |
| 426 |   426   | 0.008000496254448966  |
| 409 |   409   |  0.00789822302328142  |
| 574 |   574   | 0.0075884950306948085 |
| 239 |   239   | 0.007412513801825009  |
| 477 |   477   | 0.007402308764548453  |
| 116 |   116   | 0.0071808420899208344 |
| 494 |   494   | 0.007086555445596247  |
+-----+---------+-----------------------+


--------------------------------------------------------------------------------
MODEL TUNING EVALUATION:


  Accuracy: 0.9299363057324841


  Classification report:               precision    recall  f1-score   support

          -1       0.93      1.00      0.96       437
           1       1.00      0.03      0.06        34

    accuracy                           0.93       471
   macro avg       0.96      0.51      0.51       471
weighted avg       0.93      0.93      0.90       471


--------------------------------------------------------------------------------
MODEL SAVING:


  Model path: /Users/Barathi/SECOM-Dissertation-CODE/artifacts/models/GradientBoostingClassifier_2024-08-30_09-58-47_model.pkl

--------------------------------------------------------------------------------
SFS:


  Status: Skipped

--------------------------------------------------------------------------------
================================================================================