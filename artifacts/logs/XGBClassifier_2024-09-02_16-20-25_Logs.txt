==========================================================================================
          Semiconductor Manufacturing Unit (SECOM) Failure Prediction LOG REPORT          
==========================================================================================

Log created at: 2024-09-02 16:19:39

Model Utilised for the Experiment: XGBClassifier

==========================================================================================
DATA LOADING:


  Data loaded: True


  Total samples: 1567


  Total features: 592


  Missing values: Yes


  Dataframe info: <class 'pandas.core.frame.DataFrame'>
RangeIndex: 1567 entries, 0 to 1566
Columns: 592 entries, Time to Pass/Fail
dtypes: float64(590), int64(1), object(1)
memory usage: 7.1+ MB


--------------------------------------------------------------------------------
DATA PROCESSING:


  Missing values:

    Time: 0
    0: 6
    1: 7
    2: 14
    3: 14
    4: 14
    5: 14
    6: 14
    7: 9
    8: 2
    9: 2
    10: 2
    11: 2
    12: 2
    13: 3
    14: 3
    15: 3
    16: 3
    17: 3
    18: 3
    19: 10
    20: 0
    21: 2
    22: 2
    23: 2
    24: 2
    25: 2
    26: 2
    27: 2
    28: 2
    29: 2
    30: 2
    31: 2
    32: 1
    33: 1
    34: 1
    35: 1
    36: 1
    37: 1
    38: 1
    39: 1
    40: 24
    41: 24
    42: 1
    43: 1
    44: 1
    45: 1
    46: 1
    47: 1
    48: 1
    49: 1
    50: 1
    51: 1
    52: 1
    53: 4
    54: 4
    55: 4
    56: 4
    57: 4
    58: 4
    59: 7
    60: 6
    61: 6
    62: 6
    63: 7
    64: 7
    65: 7
    66: 6
    67: 6
    68: 6
    69: 6
    70: 6
    71: 6
    72: 794
    73: 794
    74: 6
    75: 24
    76: 24
    77: 24
    78: 24
    79: 24
    80: 24
    81: 24
    82: 24
    83: 1
    84: 12
    85: 1341
    86: 0
    87: 0
    88: 0
    89: 51
    90: 51
    91: 6
    92: 2
    93: 2
    94: 6
    95: 6
    96: 6
    97: 6
    98: 6
    99: 6
    100: 6
    101: 6
    102: 6
    103: 2
    104: 2
    105: 6
    106: 6
    107: 6
    108: 6
    109: 1018
    110: 1018
    111: 1018
    112: 715
    113: 0
    114: 0
    115: 0
    116: 0
    117: 0
    118: 24
    119: 0
    120: 0
    121: 9
    122: 9
    123: 9
    124: 9
    125: 9
    126: 9
    127: 9
    128: 9
    129: 9
    130: 9
    131: 9
    132: 8
    133: 8
    134: 8
    135: 5
    136: 6
    137: 7
    138: 14
    139: 14
    140: 14
    141: 14
    142: 14
    143: 9
    144: 2
    145: 2
    146: 2
    147: 2
    148: 2
    149: 3
    150: 3
    151: 3
    152: 3
    153: 3
    154: 3
    155: 10
    156: 0
    157: 1429
    158: 1429
    159: 2
    160: 2
    161: 2
    162: 2
    163: 2
    164: 2
    165: 2
    166: 2
    167: 2
    168: 2
    169: 2
    170: 1
    171: 1
    172: 1
    173: 1
    174: 1
    175: 1
    176: 1
    177: 1
    178: 24
    179: 1
    180: 1
    181: 1
    182: 1
    183: 1
    184: 1
    185: 1
    186: 1
    187: 1
    188: 1
    189: 1
    190: 4
    191: 4
    192: 4
    193: 4
    194: 4
    195: 4
    196: 7
    197: 6
    198: 6
    199: 6
    200: 7
    201: 7
    202: 7
    203: 6
    204: 6
    205: 6
    206: 6
    207: 6
    208: 6
    209: 6
    210: 24
    211: 24
    212: 24
    213: 24
    214: 24
    215: 24
    216: 24
    217: 24
    218: 1
    219: 12
    220: 1341
    221: 0
    222: 0
    223: 0
    224: 51
    225: 51
    226: 6
    227: 2
    228: 2
    229: 6
    230: 6
    231: 6
    232: 6
    233: 6
    234: 6
    235: 6
    236: 6
    237: 6
    238: 2
    239: 2
    240: 6
    241: 6
    242: 6
    243: 6
    244: 1018
    245: 1018
    246: 1018
    247: 715
    248: 0
    249: 0
    250: 0
    251: 0
    252: 0
    253: 24
    254: 0
    255: 0
    256: 9
    257: 9
    258: 9
    259: 9
    260: 9
    261: 9
    262: 9
    263: 9
    264: 9
    265: 9
    266: 9
    267: 8
    268: 8
    269: 8
    270: 5
    271: 6
    272: 7
    273: 14
    274: 14
    275: 14
    276: 14
    277: 14
    278: 9
    279: 2
    280: 2
    281: 2
    282: 2
    283: 2
    284: 3
    285: 3
    286: 3
    287: 3
    288: 3
    289: 3
    290: 10
    291: 0
    292: 1429
    293: 1429
    294: 2
    295: 2
    296: 2
    297: 2
    298: 2
    299: 2
    300: 2
    301: 2
    302: 2
    303: 2
    304: 2
    305: 1
    306: 1
    307: 1
    308: 1
    309: 1
    310: 1
    311: 1
    312: 1
    313: 24
    314: 24
    315: 1
    316: 1
    317: 1
    318: 1
    319: 1
    320: 1
    321: 1
    322: 1
    323: 1
    324: 1
    325: 1
    326: 4
    327: 4
    328: 4
    329: 4
    330: 4
    331: 4
    332: 7
    333: 6
    334: 6
    335: 6
    336: 7
    337: 7
    338: 7
    339: 6
    340: 6
    341: 6
    342: 6
    343: 6
    344: 6
    345: 794
    346: 794
    347: 6
    348: 24
    349: 24
    350: 24
    351: 24
    352: 24
    353: 24
    354: 24
    355: 24
    356: 1
    357: 12
    358: 1341
    359: 0
    360: 0
    361: 0
    362: 51
    363: 51
    364: 6
    365: 2
    366: 2
    367: 6
    368: 6
    369: 6
    370: 6
    371: 6
    372: 6
    373: 6
    374: 6
    375: 6
    376: 2
    377: 2
    378: 6
    379: 6
    380: 6
    381: 6
    382: 1018
    383: 1018
    384: 1018
    385: 715
    386: 0
    387: 0
    388: 0
    389: 0
    390: 0
    391: 24
    392: 0
    393: 0
    394: 9
    395: 9
    396: 9
    397: 9
    398: 9
    399: 9
    400: 9
    401: 9
    402: 9
    403: 9
    404: 9
    405: 8
    406: 8
    407: 8
    408: 5
    409: 6
    410: 7
    411: 14
    412: 14
    413: 14
    414: 14
    415: 14
    416: 9
    417: 2
    418: 2
    419: 2
    420: 2
    421: 2
    422: 3
    423: 3
    424: 3
    425: 3
    426: 3
    427: 3
    428: 10
    429: 0
    430: 2
    431: 2
    432: 2
    433: 2
    434: 2
    435: 2
    436: 2
    437: 2
    438: 2
    439: 2
    440: 2
    441: 1
    442: 1
    443: 1
    444: 1
    445: 1
    446: 1
    447: 1
    448: 1
    449: 24
    450: 24
    451: 1
    452: 1
    453: 1
    454: 1
    455: 1
    456: 1
    457: 1
    458: 1
    459: 1
    460: 1
    461: 1
    462: 4
    463: 4
    464: 4
    465: 4
    466: 4
    467: 4
    468: 7
    469: 6
    470: 6
    471: 6
    472: 7
    473: 7
    474: 7
    475: 6
    476: 6
    477: 6
    478: 6
    479: 6
    480: 6
    481: 6
    482: 24
    483: 24
    484: 24
    485: 24
    486: 24
    487: 24
    488: 24
    489: 24
    490: 1
    491: 12
    492: 1341
    493: 0
    494: 0
    495: 0
    496: 51
    497: 51
    498: 6
    499: 2
    500: 2
    501: 6
    502: 6
    503: 6
    504: 6
    505: 6
    506: 6
    507: 6
    508: 6
    509: 6
    510: 2
    511: 2
    512: 6
    513: 6
    514: 6
    515: 6
    516: 1018
    517: 1018
    518: 1018
    519: 715
    520: 0
    521: 0
    522: 0
    523: 0
    524: 0
    525: 24
    526: 0
    527: 0
    528: 9
    529: 9
    530: 9
    531: 9
    532: 9
    533: 9
    534: 9
    535: 9
    536: 9
    537: 9
    538: 9
    539: 8
    540: 8
    541: 8
    542: 2
    543: 2
    544: 2
    545: 2
    546: 260
    547: 260
    548: 260
    549: 260
    550: 260
    551: 260
    552: 260
    553: 260
    554: 260
    555: 260
    556: 260
    557: 260
    558: 1
    559: 1
    560: 1
    561: 1
    562: 273
    563: 273
    564: 273
    565: 273
    566: 273
    567: 273
    568: 273
    569: 273
    570: 0
    571: 0
    572: 0
    573: 0
    574: 0
    575: 0
    576: 0
    577: 0
    578: 949
    579: 949
    580: 949
    581: 949
    582: 1
    583: 1
    584: 1
    585: 1
    586: 1
    587: 1
    588: 1
    589: 1
    Pass/Fail: 0


  Categorical columns: ['Time']


  Numeric columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', 'Pass/Fail']


  Other columns: None


  Imputation method type: global_imputation


  Imputation methods: fillna


  Feature engineering: Date columns separated


  One hot encoded columns: No one-hot encoding performed.


  Dropped columns: ['Time']


  Final number of columns: 597


  Final columns: Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
       ...
       '587', '588', '589', 'Pass/Fail', 'Year', 'Month', 'Day', 'Hour',
       'Day of Week', 'Minute'],
      dtype='object', length=597)

--------------------------------------------------------------------------------
TRAIN TEST SPLIT:


  Target column: Pass/Fail


  Test size: 0.3


  Train size: 1096


  Test size count: 471


  Scaling: None


  Checks: All checks passed successfully

--------------------------------------------------------------------------------
MODEL TRAINING:


  Model was opted to handle missing values: False


  Model name: XGBClassifier


  Training started: 2024-09-02 16:19:39


  Training completed: 2024-09-02 16:19:40


  Training duration: 0.57 seconds


  Status: completed


  Feature importance calculation status: started


  Feature importance technique: Plot Tree Importance


  Feature importance calculation: completed


  Number of features selected: 35


  Feature importance: 
+-----+---------+----------------------+
|     | Feature |      Importance      |
+-----+---------+----------------------+
| 122 |   122   | 0.020526090636849403 |
| 267 |   267   | 0.019171077758073807 |
| 28  |   28    | 0.018291790038347244 |
| 100 |   100   | 0.016750501468777657 |
| 164 |   164   | 0.016175227239727974 |
| 40  |   40    | 0.016040561720728874 |
| 386 |   386   | 0.01580432429909706  |
| 156 |   156   | 0.015490717254579067 |
| 277 |   277   | 0.015070958994328976 |
| 591 |  Month  | 0.01267961971461773  |
| 353 |   353   | 0.012586291879415512 |
| 485 |   485   | 0.012370620854198933 |
| 81  |   81    | 0.012152117677032948 |
| 25  |   25    | 0.01211071852594614  |
| 519 |   519   | 0.012052599340677261 |
| 572 |   572   | 0.01185676921159029  |
| 305 |   305   | 0.011787579394876957 |
| 64  |   64    | 0.011151698417961597 |
| 214 |   214   | 0.010907408781349659 |
| 390 |   390   | 0.010872475802898407 |
| 132 |   132   | 0.010702753439545631 |
| 120 |   120   | 0.010657303966581821 |
| 348 |   348   | 0.01052907109260559  |
| 11  |   11    | 0.010203440673649311 |
| 59  |   59    | 0.010167877189815044 |
| 93  |   93    | 0.009946574456989765 |
| 365 |   365   | 0.009359861724078655 |
| 571 |   571   | 0.009304888546466827 |
| 555 |   555   | 0.009167483076453209 |
| 304 |   304   | 0.008891922421753407 |
| 363 |   363   | 0.008880079723894596 |
| 500 |   500   | 0.008845388889312744 |
| 246 |   246   | 0.008797367103397846 |
| 38  |   38    | 0.008787714876234531 |
| 345 |   345   | 0.00877396296709776  |
+-----+---------+----------------------+


--------------------------------------------------------------------------------
MODEL EVALUATION:


  Accuracy: 0.9320594479830149


  Classification report:               precision    recall  f1-score   support

           0       0.93      1.00      0.96       437
           1       1.00      0.06      0.11        34

    accuracy                           0.93       471
   macro avg       0.97      0.53      0.54       471
weighted avg       0.94      0.93      0.90       471


--------------------------------------------------------------------------------
MODEL TUNING:


  Tuning started: 2024-09-02 16:19:40


  Method: Optuna for hyperparameter tuning


  N trials: 100


  Best hyperparameters:

    n_estimators: 105
    learning_rate: 0.4757790352278735
    max_depth: 3
    min_child_weight: 2
    subsample: 0.8292896750174519
    colsample_bytree: 0.9368705330678105
    gamma: 0.33974468746324826
    reg_alpha: 0.7358669094668824
    reg_lambda: 0.2057422296991001


  Tuning completed: 2024-09-02 16:20:25


  Tuning duration: 44.78 seconds


  Status: completed


  Feature importance calculation status: started


  Feature importance technique: Plot Tree Importance


  Feature importance calculation: completed


  Number of features selected: 35


  Feature importance: 
+-----+---------+----------------------+
|     | Feature |      Importance      |
+-----+---------+----------------------+
| 511 |   511   |  0.0366302952170372  |
| 57  |   57    | 0.025912055745720863 |
| 122 |   122   | 0.025202617049217224 |
| 164 |   164   | 0.02425486221909523  |
| 64  |   64    | 0.024127669632434845 |
| 361 |   361   | 0.022669395431876183 |
| 584 |   584   | 0.02245286852121353  |
| 59  |   59    | 0.022334277629852295 |
| 227 |   227   | 0.02143840491771698  |
| 297 |   297   | 0.02114684507250786  |
| 519 |   519   | 0.020774802193045616 |
| 301 |   301   | 0.020466940477490425 |
| 575 |   575   | 0.019871195778250694 |
| 500 |   500   | 0.019501421600580215 |
| 146 |   146   | 0.01883542723953724  |
| 345 |   345   | 0.018391303718090057 |
| 121 |   121   | 0.01737920381128788  |
| 168 |   168   | 0.016754066571593285 |
| 582 |   582   | 0.01626943238079548  |
| 587 |   587   | 0.016193950548768044 |
| 488 |   488   | 0.016039738431572914 |
| 348 |   348   | 0.01597519963979721  |
| 405 |   405   | 0.01572456583380699  |
| 426 |   426   | 0.015656111761927605 |
| 333 |   333   | 0.015095123089849949 |
| 479 |   479   | 0.014239795506000519 |
| 82  |   82    | 0.013785541988909245 |
| 271 |   271   | 0.01355838030576706  |
| 319 |   319   | 0.013426470570266247 |
| 25  |   25    | 0.011563718318939209 |
| 386 |   386   | 0.01149670872837305  |
| 21  |   21    | 0.011493178084492683 |
| 81  |   81    | 0.011039407923817635 |
| 16  |   16    | 0.010992019437253475 |
| 298 |   298   | 0.010973324999213219 |
+-----+---------+----------------------+


--------------------------------------------------------------------------------
MODEL TUNING EVALUATION:


  Accuracy: 0.9363057324840764


  Classification report:               precision    recall  f1-score   support

           0       0.94      1.00      0.97       437
           1       0.83      0.15      0.25        34

    accuracy                           0.94       471
   macro avg       0.89      0.57      0.61       471
weighted avg       0.93      0.94      0.92       471


--------------------------------------------------------------------------------
MODEL SAVING:


  Model path: /Users/karthik/SECOM-Dissertation-CODE/artifacts/models/XGBClassifier_2024-09-02_16-20-25_model.pkl

--------------------------------------------------------------------------------
SFS:


  Status: Skipped

--------------------------------------------------------------------------------
================================================================================